# -*- coding: utf-8 -*-
"""Cópia de CNN_Original.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H0DJSSbHZZIp5-HuyWGOXl2_Mc4NZ2si
"""

#!sudo update-alternatives --config python3
#!sudo apt install python3-pip

import pandas as pd
import torch
import random
import numpy as np
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize

#import torchtext.vocab
import tqdm
import torch.nn as nn
import torch.nn.functional as F

import pickle
from nltk import tokenize
import torch.optim as optim

from sklearn.metrics import classification_report
import torchtext

#from torchtext.data import Field, LabelField, TabularDataset, BucketIterator

#from torchtext.vocab import GloVe
#from torchtext.data import BucketIterator

# from torchtext.datasets import TabularDataset
# from torchtext.data import Field, LabelField
from torch.utils.data import DataLoader, Dataset

import gensim

path = r"C:\Users\victo\Downloads\corpus\labeled"

class TextDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=512):
        self.data = pd.read_csv(csv_file)  # Lê o CSV com pandas
        self.tokenizer = tokenizer
        self.max_length = max_length  # Defina o comprimento máximo da sequência
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # Recupera o texto da linha correspondente
        text = self.data.iloc[idx]['text']
        
        # Verifica se o texto é um float (ou NaN) e trata
        if isinstance(text, float) or pd.isna(text):  # Se for float ou NaN, converta para string vazia
            text = ""  # Ou use str(text) se preferir
        else:
            text = str(text)  # Certifique-se de que é uma string

        # Tokeniza o texto com padding automático
        tokenized_text = self.tokenizer(text)

        # Recupera a ajuda (ou outra coluna que você precisa)
        label = torch.tensor(self.data.iloc[idx]['helpfulness'], dtype=torch.float)

        return tokenized_text['input_ids'].squeeze(0), label

### Reprodutibilidade ###
SEED = 1234

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
#########################
word_to_index = {"<pad>": 0}  # Inicializa o dicionário com o token de padding

# def tokenizer(text):
#     # Tokeniza o texto usando o NLTK
#     tokenized = tokenize.word_tokenize(text, language='portuguese')
    
#     # Atualiza o dicionário word_to_index com os novos tokens
#     for token in tokenized:
#         if token not in word_to_index:
#             word_to_index[token] = len(word_to_index)  # Atribui um índice incremental

#     # Se o número de tokens for menor que 5, preenche com <pad>
#     if len(tokenized) < 5:
#         tokenized += ['<pad>'] * (5 - len(tokenized))
    
#     return tokenized

tokenizer = lambda text: word_tokenize(text, language='portuguese')

def collate_fn(batch):
    # Colapsa os dados em lotes
    input_ids = torch.stack([item[0] for item in batch])  # Empilha os input_ids
    labels = torch.stack([item[1] for item in batch])  # Empilha os rótulos
    return input_ids, labels

def process_splits(path, BATCH_SIZE, device, glove_path):
    nltk.download('punkt')  # Certifique-se de que o recurso 'punkt' está disponível
    # Usando o tokenizador para português
#    tokenizer = lambda text: word_tokenize(text, language='portuguese')
    # Criando os datasets
    train_dataset = TextDataset(path + '/balanced_train_apps.csv', tokenizer)
    valid_dataset = TextDataset(path + '/balanced_dev_apps.csv', tokenizer)
    test_dataset = TextDataset(path + '/balanced_test_apps.csv', tokenizer)

    # Carregando os embeddings do GloVe (arquivo local)
    print("Carregando os embeddings do GloVe...")
    embeddings = gensim.models.KeyedVectors.load_word2vec_format(glove_path, binary=False)

    # Criando os DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_fn)
    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False,collate_fn=collate_fn)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,collate_fn=collate_fn)

    return train_loader, valid_loader, test_loader, embeddings

# Defina os parâmetros antes de chamar a função
BATCH_SIZE = 64
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
glove_path = f"{path}/glove_s300.txt"  # Defina o caminho correto

train_iterator, valid_iterator, test_iterator, embeddings = process_splits(path, BATCH_SIZE, device, glove_path)

# Teste se os embeddings carregaram corretamente
print(f"Palavras no vocabulário: {len(embeddings.index_to_key)}")
print(f"Dimensão dos vetores: {embeddings.vector_size}")

class CNN_Multitask(nn.Module):
    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim_polarity, 
                 output_dim_utility, dropout, vectors):
        super().__init__()

        # Camada de Embeddings
        self.embedding = nn.Embedding.from_pretrained(vectors, freeze=True)

        # 3 camadas de convolução
        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))
        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))
        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))

        # A saída dos filtros alimenta uma MLP
        self.fc = nn.Linear(len(filter_sizes) * n_filters, 32)
        
        # Camadas de saída para cada task
        self.fc_polarity = nn.Linear(32, output_dim_polarity)  # Para a polaridade
        self.fc_utility = nn.Linear(32, output_dim_utility)    # Para a utilidade da opinião

        # Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):
        # text = [batch size, sent len]

        embedded = self.embedding(text)
        # embedded = [batch size, sent len, emb dim]

        embedded = embedded.unsqueeze(1)
        # embedded = [batch size, 1, sent len, emb dim]

        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))
        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))
        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))

        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]

        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)
        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)
        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)

        # pooled_n = [batch size, n_filters]

        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))
        # cat = [batch size, n_filters * len(filter_sizes)]

        # Passa pela camada MLP
        out = F.relu(self.fc(cat))

        # Saídas para as duas tasks
        polarity_output = self.fc_polarity(out)  # Polaridade: binária
        utility_output = self.fc_utility(out)    # Utilidade da opinião: binária

        return polarity_output, utility_output

INPUT_DIM = len(embeddings.key_to_index)  # Tamanho do vocabulário
EMBEDDING_DIM = 300  # Dimensão dos vetores
N_FILTERS = 100
FILTER_SIZES = [3,4,5]
OUTPUT_DIM = 1
DROPOUT = 0.7

# Adicionando token de padding
PAD_TOKEN = "<pad>"
UNK_TOKEN = "<unk>"

embeddings.add_vector(PAD_TOKEN, np.zeros(EMBEDDING_DIM))
embeddings.add_vector(UNK_TOKEN, np.zeros(EMBEDDING_DIM))

PAD_IDX = embeddings.key_to_index[PAD_TOKEN]
UNK_IDX = embeddings.key_to_index[UNK_TOKEN]

# Convertendo vetores para PyTorch
VECTORS = torch.tensor(embeddings.vectors, dtype=torch.float)

def binary_accuracy(preds, y):

    #leva o valor para o inteiro mais próxio
    rounded_preds = torch.round(torch.sigmoid(preds))
    correct = (rounded_preds == y).float()
    acc = correct.sum() / len(correct)
    return acc

# Criando o modelo
model = CNN_Multitask(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES,1,1, DROPOUT, VECTORS)

# Inicializando embeddings especiais
if UNK_TOKEN in embeddings.key_to_index:
    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)

model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)

# Enviando modelo para o dispositivo correto
model = model.to(device)

# Definindo otimizador e função de perda
optimizer = optim.Adam(model.parameters())
criterion = nn.BCEWithLogitsLoss()
criterion = criterion.to(device)

def train(model, iterator, optimizer, criterion):
    epoch_loss = 0
    epoch_acc = 0
    model.train()

    for batch in tqdm.tqdm(iterator, desc='training...'):
        texts = batch[0]  # Entradas (textos)
        helpfulness = batch[1]  # Rótulos de ajuda (ou utilidade)

        optimizer.zero_grad()

        # Fazendo as previsões para as duas tarefas
        polarity_output, utility_output = model(texts.to(device))

        # Calcule a perda e a acurácia
        loss_polarity = criterion(polarity_output.squeeze(1), helpfulness.to(device))  # Squeeze se necessário
        loss_utility = criterion(utility_output.squeeze(1), helpfulness.to(device))  # Squeeze se necessário
        loss = loss_polarity + loss_utility  # Combinação das perdas

        acc_polarity = binary_accuracy(polarity_output, helpfulness.to(device))
        acc_utility = binary_accuracy(utility_output, helpfulness.to(device))
        acc = (acc_polarity + acc_utility) / 2  # Média das acurácias das duas tarefas

        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator)

def evaluate(model, iterator, criterion):
    epoch_loss = 0
    epoch_acc = 0
    total_predictions = []
    model.eval()

    with torch.no_grad():
        for batch in tqdm.tqdm(iterator, desc='evaluating...'):
            texts = batch[0]  # Entradas
            helpfulness = batch[1]  # Rótulos

            # Fazendo as previsões para as duas tarefas
            polarity_output, utility_output = model(texts.to(device))

            # Armazenando as previsões
            total_predictions.extend(p.item() for p in polarity_output)

            # Calcule a perda e a acurácia
            loss_polarity = criterion(polarity_output.squeeze(1), helpfulness.to(device))  # Squeeze se necessário
            loss_utility = criterion(utility_output.squeeze(1), helpfulness.to(device))  # Squeeze se necessário
            loss = loss_polarity + loss_utility

            acc_polarity = binary_accuracy(polarity_output, helpfulness.to(device))
            acc_utility = binary_accuracy(utility_output, helpfulness.to(device))
            acc = (acc_polarity + acc_utility) / 2

            epoch_loss += loss.item()
            epoch_acc += acc.item()

    return epoch_loss / len(iterator), epoch_acc / len(iterator), total_predictions# Treinamento do modelo
N_EPOCHS = 5
best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)
    valid_loss, valid_acc, _  = evaluate(model, valid_iterator, criterion)

    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'cnn_multitask_apps.pt')

    print(f'Epoch: {epoch+1:02}')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')

# Carregar melhor modelo
# model.load_state_dict(torch.load('cnn_multitask_apps.pt'))

# # Avaliação final
# test_loss, test_acc, predictions = evaluate(model, test_iterator, criterion)
# print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')

# # Classificação final
# preds = torch.FloatTensor(predictions)
# rounded_preds = torch.round(torch.sigmoid(preds))

# labels = [float(t.helpfulness) for t in test_iterator]
# print(classification_report(labels, rounded_preds.detach().numpy()))



"""## apps results

evaluating...: 100%|██████████| 681/681 [00:02<00:00, 325.29it/s]

Test Loss: 0.549 | Test Acc: 72.82%
              precision    recall  f1-score   support

         0.0       0.98      0.82      0.89     10882
         1.0       0.85      0.98      0.91     10882

    accuracy                           0.90     21764
   macro avg       0.91      0.90      0.90     21764
weighted avg       0.91      0.90      0.90     21764

Filmes Results

evaluating...: 100%|██████████| 4777/4777 [00:24<00:00, 193.09it/s]

Test Loss: 0.633 | Test Acc: 63.96%
              precision    recall  f1-score   support

         0.0       0.79      0.67      0.72     76418
         1.0       0.71      0.82      0.76     76418

    accuracy                           0.74    152836
   macro avg       0.75      0.74      0.74    152836
weighted avg       0.75      0.74      0.74    152836
"""

##rodar apenas a avaliação fazer o load do modelo e rodar apenas a avaliação.